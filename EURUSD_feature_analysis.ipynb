{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import auc, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('Data/random_split/Train.csv')\n",
    "y_train = X_train['up_down']\n",
    "X_train = X_train.drop(columns = ['up_down','Unnamed: 0'])\n",
    "\n",
    "X_test = pd.read_csv('Data/random_split/Test_features.csv')\n",
    "X_test = X_test.drop(columns = ['Unnamed: 0'])\n",
    "y_test = pd.read_csv('Data/random_split/Test_label.csv')\n",
    "y_test = y_test.drop(columns = ['num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling Volume\n",
    "import sklearn.preprocessing as preprocessing\n",
    "scaler = preprocessing.StandardScaler()\n",
    "vol_scale_param = scaler.fit(X_train['Volume'].values.reshape(-1,1))\n",
    "\n",
    "X_train['Volume_scaled'] = scaler.fit_transform(X_train['Volume'].values.reshape(-1,1), vol_scale_param)\n",
    "X_train = X_train.drop(columns = ['Volume'])\n",
    "\n",
    "X_test['Volume_scaled'] = scaler.fit_transform(X_test['Volume'].values.reshape(-1,1), vol_scale_param)\n",
    "X_test = X_test.drop(columns = ['Volume'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "select = RFE(RandomForestClassifier(n_estimators=100, random_state=42), n_features_to_select=40)\n",
    "#select = RFE(LogisticRegression(penalty=\"l1\"), n_features_to_select=40)\n",
    "\n",
    "select.fit(X_train, y_train)\n",
    "# visualize the selected features:\n",
    "mask = select.get_support()\n",
    "plt.matshow(mask.reshape(1, -1), cmap='gray_r')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction using GDBT of lightGBM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'max_depth': 5, 'min_child_weight': 5}, 0.746003583989124)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test = {\n",
    "    'max_depth':np.arange(3,10,2),\n",
    "    'min_child_weight':np.arange(1,6,2)\n",
    "}\n",
    "\n",
    "estimator = lgb.LGBMClassifier(boosting_type='gbdt', \n",
    "                           objective='binary',\n",
    "                           metric='auc',\n",
    "                           num_leaves=9,\n",
    "                           min_split_gain=0,\n",
    "                           min_child_weight=5,\n",
    "                           min_child_samples=10,\n",
    "                           max_bin=100,\n",
    "                           subsample= 0.7,  # Subsample ratio of the training instance.\n",
    "                           subsample_freq=1,  # frequence of subsample, <=0 means no enable\\\n",
    "                           colsample_bytree=0.7,  # Subsample ratio of columns when constructing each tree.\n",
    "                           reg_alpha=1, \n",
    "                           reg_lambda=0,\n",
    "                           seed=410, \n",
    "                           nthread=4, \n",
    "                           silent=True)\n",
    "\n",
    "\n",
    "gsearch = GridSearchCV(estimator, param_grid = param_test, scoring='roc_auc',n_jobs=1,iid=False, cv=5)\n",
    "\n",
    "lgb_model = gsearch.fit(X_train.values, y_train.values)\n",
    "lgb_model.best_params_, lgb_model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'max_depth': 6, 'min_child_weight': 6}, 0.7460723437130179)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test = {\n",
    "    'max_depth':[4,5,6],\n",
    "    'min_child_weight':[4,5,6]\n",
    "}\n",
    "\n",
    "estimator = lgb.LGBMClassifier(boosting_type='gbdt', \n",
    "                           objective='binary',\n",
    "                           metric='auc',\n",
    "                           num_leaves=9,\n",
    "                           min_split_gain=0,\n",
    "                           min_child_weight=5,\n",
    "                           min_child_samples=10,\n",
    "                           max_bin=100,\n",
    "                           subsample= 0.7,  # Subsample ratio of the training instance.\n",
    "                           subsample_freq=1,  # frequence of subsample, <=0 means no enable\\\n",
    "                           colsample_bytree=0.7,  # Subsample ratio of columns when constructing each tree.\n",
    "                           reg_alpha=1, \n",
    "                           reg_lambda=0,\n",
    "                           seed=410, \n",
    "                           nthread=4, \n",
    "                           silent=True)\n",
    "\n",
    "\n",
    "gsearch = GridSearchCV(estimator, param_grid = param_test, scoring='roc_auc',n_jobs=1,iid=False, cv=5)\n",
    "\n",
    "lgb_model = gsearch.fit(X_train.values, y_train.values)\n",
    "lgb_model.best_params_, lgb_model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'num_leaves': 23}, 0.746430961779909)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test = {\n",
    "    'num_leaves':np.arange(3,100,5)\n",
    "}\n",
    "\n",
    "estimator = lgb.LGBMClassifier(boosting_type='gbdt', \n",
    "                           objective='binary',\n",
    "                           metric='auc',\n",
    "                           max_depth=6,\n",
    "                           min_split_gain=0,\n",
    "                           min_child_weight=6,\n",
    "                           min_child_samples=10,\n",
    "                           max_bin=100,\n",
    "                           subsample= 0.7,  # Subsample ratio of the training instance.\n",
    "                           subsample_freq=1,  # frequence of subsample, <=0 means no enable\\\n",
    "                           colsample_bytree=0.7,  # Subsample ratio of columns when constructing each tree.\n",
    "                           reg_alpha=1, \n",
    "                           reg_lambda=0,\n",
    "                           seed=410, \n",
    "                           nthread=4, \n",
    "                           silent=True)\n",
    "\n",
    "\n",
    "gsearch = GridSearchCV(estimator, param_grid = param_test, scoring='roc_auc',n_jobs=1,iid=False, cv=5)\n",
    "\n",
    "lgb_model = gsearch.fit(X_train.values, y_train.values)\n",
    "lgb_model.best_params_, lgb_model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'max_bin': 100}, 0.746430961779909)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test = {\n",
    "    'max_bin':[i*100 for i in range(1,5)]\n",
    "}\n",
    "\n",
    "estimator = lgb.LGBMClassifier(boosting_type='gbdt', \n",
    "                           objective='binary',\n",
    "                           metric='auc',\n",
    "                           num_leaves=23,\n",
    "                           max_depth=6,\n",
    "                           min_split_gain=0,\n",
    "                           min_child_weight=6,\n",
    "                           min_child_samples=10,\n",
    "                           max_bin=100,\n",
    "                           subsample= 0.7,  # Subsample ratio of the training instance.\n",
    "                           subsample_freq=1,  # frequence of subsample, <=0 means no enable\\\n",
    "                           colsample_bytree=0.7,  # Subsample ratio of columns when constructing each tree.\n",
    "                           reg_alpha=1, \n",
    "                           reg_lambda=0,\n",
    "                           seed=410, \n",
    "                           nthread=4, \n",
    "                           silent=True)\n",
    "\n",
    "\n",
    "gsearch = GridSearchCV(estimator, param_grid = param_test, scoring='roc_auc',n_jobs=1,iid=False, cv=5)\n",
    "\n",
    "lgb_model = gsearch.fit(X_train.values, y_train.values)\n",
    "lgb_model.best_params_, lgb_model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'colsample_bytree': 0.5, 'subsample': 0.9}, 0.7466614557273576)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test = {\n",
    "    'subsample':[i/10.0 for i in range(4,10)],\n",
    "    'colsample_bytree':[i/10.0 for i in range(4,10)]\n",
    "}\n",
    "\n",
    "estimator = lgb.LGBMClassifier(boosting_type='gbdt', \n",
    "                           objective='binary',\n",
    "                           metric='auc',\n",
    "                           num_leaves=23,\n",
    "                           max_depth=6,\n",
    "                           min_split_gain=0,\n",
    "                           min_child_weight=6,\n",
    "                           min_child_samples=10,\n",
    "                           max_bin=100,\n",
    "                           subsample= 0.7,  # Subsample ratio of the training instance.\n",
    "                           subsample_freq=1,  # frequence of subsample, <=0 means no enable\\\n",
    "                           colsample_bytree=0.7,  # Subsample ratio of columns when constructing each tree.\n",
    "                           reg_alpha=1, \n",
    "                           reg_lambda=0,\n",
    "                           seed=410, \n",
    "                           nthread=4, \n",
    "                           silent=True)\n",
    "\n",
    "\n",
    "gsearch = GridSearchCV(estimator, param_grid = param_test, scoring='roc_auc',n_jobs=1,iid=False, cv=5)\n",
    "\n",
    "lgb_model = gsearch.fit(X_train.values, y_train.values)\n",
    "lgb_model.best_params_, lgb_model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'reg_alpha': 0.5, 'reg_lambda': 0.1}, 0.746766917332304)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test = {\n",
    "    'reg_alpha':[0, 1e-5, 1e-2, 0.1, 0.5, 1],\n",
    "    'reg_lambda':[0, 1e-5, 1e-2, 0.1, 0.5, 1]\n",
    "}\n",
    "\n",
    "estimator = lgb.LGBMClassifier(boosting_type='gbdt', \n",
    "                           objective='binary',\n",
    "                           metric='auc',\n",
    "                           num_leaves=23,\n",
    "                           max_depth=6,\n",
    "                           min_split_gain=0,\n",
    "                           min_child_weight=6,\n",
    "                           min_child_samples=10,\n",
    "                           max_bin=100,\n",
    "                           subsample= 0.9,  # Subsample ratio of the training instance.\n",
    "                           subsample_freq=1,  # frequence of subsample, <=0 means no enable\\\n",
    "                           colsample_bytree=0.5,  # Subsample ratio of columns when constructing each tree.\n",
    "                           reg_alpha=1, \n",
    "                           reg_lambda=0,\n",
    "                           seed=410, \n",
    "                           nthread=4, \n",
    "                           silent=True)\n",
    "\n",
    "\n",
    "gsearch = GridSearchCV(estimator, param_grid = param_test, scoring='roc_auc',n_jobs=1,iid=False, cv=5)\n",
    "\n",
    "lgb_model = gsearch.fit(X_train.values, y_train.values)\n",
    "lgb_model.best_params_, lgb_model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'learning_rate': 0.05}, 0.7470489968799185)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test = {\n",
    "    'learning_rate': [0.05,0.1, 0.2, 0.5, 1, 2],\n",
    "}\n",
    "\n",
    "estimator = lgb.LGBMClassifier(boosting_type='gbdt', \n",
    "                           objective='binary',\n",
    "                           metric='auc',\n",
    "                           learning_rate=0.05,\n",
    "                           num_leaves=23,\n",
    "                           max_depth=6,\n",
    "                           min_split_gain=0,\n",
    "                           min_child_weight=6,\n",
    "                           min_child_samples=10,\n",
    "                           max_bin=100,\n",
    "                           subsample= 0.9,  # Subsample ratio of the training instance.\n",
    "                           subsample_freq=1,  # frequence of subsample, <=0 means no enable\\\n",
    "                           colsample_bytree=0.5,  # Subsample ratio of columns when constructing each tree.\n",
    "                           reg_alpha=0.5, \n",
    "                           reg_lambda=0.1,\n",
    "                           seed=410, \n",
    "                           nthread=4, \n",
    "                           silent=True)\n",
    "\n",
    "\n",
    "gsearch = GridSearchCV(estimator, param_grid = param_test, scoring='roc_auc',n_jobs=1,iid=False, cv=5)\n",
    "\n",
    "lgb_model = gsearch.fit(X_train.values, y_train.values)\n",
    "lgb_model.best_params_, lgb_model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:1036: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    }
   ],
   "source": [
    "lgb_params = {'boosting_type': 'gbdt',\n",
    "              'objective': 'binary',\n",
    "              'metric':'auc',\n",
    "              'learning_rate': 0.05,\n",
    "              'num_leaves': 23,\n",
    "              'max_depth': 6,\n",
    "              'min_child_samples': 10,\n",
    "              'max_bin': 100,\n",
    "              'subsample': 0.9,\n",
    "              'subsample_freq': 1,\n",
    "              'colsample_bytree': 0.5,\n",
    "              'min_child_weight': 6,\n",
    "              'min_split_gain': 0,\n",
    "              'reg_alpha': 0.5,\n",
    "              'reg_lambda': 0.1,\n",
    "              'nthread': 4,\n",
    "              'verbose': 0,\n",
    "             }\n",
    "\n",
    "predictors = list(X_train.columns)\n",
    "categorical_features = ['dayofweek','JPY','AUD','EUR','GBP','USD']\n",
    "xgtrain = lgb.Dataset(X_train[predictors].values, label=y_train.values,\n",
    "                      feature_name=predictors,\n",
    "                      categorical_feature=categorical_features\n",
    "                     )\n",
    "\n",
    "evals_results = {}\n",
    "\n",
    "lightGBM_model = lgb.train(lgb_params, \n",
    "                     xgtrain, \n",
    "                     evals_result=evals_results, \n",
    "                     verbose_eval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lightGBM_model.predict(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34987509093267316\n",
      "0.2833095545567843\n",
      "Accuracy Score: 0.6690050696727552\n"
     ]
    }
   ],
   "source": [
    "y_pred[y_pred>0.5] = 1\n",
    "y_pred[y_pred<=0.5] = 0\n",
    "\n",
    "print(sum(y_test.label.values)/len(y_test))\n",
    "print(sum(y_pred)/len(y_pred))\n",
    "print('Accuracy Score: '+ str(accuracy_score(y_test.label.values, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'lgt_GDBT_0669.sav'\n",
    "pickle.dump(lgb_model, open(filename, 'wb'))\n",
    "# some time later.. load the model from disk\n",
    "#loaded_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train.as_matrix()\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X,y_train.as_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(X_test.as_matrix())\n",
    "print('Accuracy Score: '+ str(accuracy_score(y_test.label.values, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
